# FDA Guidance Documents Harvester

A lean, functional web crawler for collecting FDA Guidance documents with full metadata and PDF downloads.

## Overview

This project crawls the FDA's guidance documents database (2,786+ documents), extracts comprehensive metadata, downloads PDFs, and stores everything in a PostgreSQL database with resume capability.

**Target Website:** https://www.fda.gov/regulatory-information/search-fda-guidance-documents

## Features

âœ… **Complete Data Extraction**
- Document metadata (title, issue date, FDA organization, topic, status)
- Detail page information (docket numbers, summaries, federal register links)
- Direct PDF downloads stored in PostgreSQL with integrity verification

âœ… **Robust Architecture**
- Async processing with configurable concurrency
- Rate limiting and polite crawling (1 req/sec default)
- Resume functionality for interrupted crawls
- Idempotent operations (no duplicates)

âœ… **Lean Implementation**
- Just 4 core Python files
- Minimal dependencies (httpx, BeautifulSoup, SQLAlchemy, Typer)
- PostgreSQL storage with proper schema
- Clean CLI interface

## Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Setup Database

```bash
# Create PostgreSQL database and run migrations
psql -h localhost -U your_user -d your_db -f migrate.sql
```

### 3. Configure

Update database connection in `fda_crawler/config.py` or set environment variables:

```bash
export DATABASE_URL="postgresql+asyncpg://user:pass@localhost:5432/db"
# Note: PDFs are stored directly in PostgreSQL, no file system storage needed
```

### 4. Test Run

```bash
# Test with 5 documents
python -m fda_crawler.cli test --limit 5

# Check status
python -m fda_crawler.cli status <session-id>

# Resume if interrupted
python -m fda_crawler.cli resume <session-id>
```

## CLI Commands

```bash
# Initialize database schema
python -m fda_crawler.cli init

# Full crawl
python -m fda_crawler.cli crawl --max-concurrency 4 --rate-limit 1.0

# Test with limited documents
python -m fda_crawler.cli test --limit 10

# Resume interrupted crawl
python -m fda_crawler.cli resume <session-id>

# Check crawl status
python -m fda_crawler.cli status <session-id>

# Export PDFs from database to files
python -m fda_crawler.cli export-pdfs --output-dir ./exported_pdfs

# View configuration
python -m fda_crawler.cli config
```

## Architecture

### Lean Design (4 Core Files)

```
fda_crawler/
â”œâ”€â”€ cli.py          # Typer CLI interface
â”œâ”€â”€ crawler.py      # Main crawling logic (HTTP + parsing + DB)
â”œâ”€â”€ models.py       # SQLAlchemy ORM models
â””â”€â”€ config.py       # Pydantic configuration
```

### Database Schema

- **crawl_sessions**: Track crawl progress and resume capability
- **documents**: FDA guidance document metadata
- **document_attachments**: PDF binary data stored directly in PostgreSQL with download tracking

### Data Flow

1. **Discovery**: Extract document URLs and metadata from FDA listing
2. **Detail Parsing**: Fetch additional metadata from individual document pages
3. **PDF Download**: Download PDF binary data with integrity verification
4. **Database Storage**: Save all data including PDF content directly to PostgreSQL

## Example Output

```
ğŸ§ª Testing crawler with 5 documents...
ğŸ“‹ Database initialized
INFO: Processing document: Medical Device User Fee Small Business Qualification...
INFO: Downloaded and stored PDF in database: 07/31/2025_medical_device_user_fee_small_business_qualificati_176439.pdf (428737 bytes)
âœ… Test crawl completed!

Session Status:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Status               â”ƒ âœ… Completed               â”ƒ
â”ƒ Progress             â”ƒ 5/5 (100.0%)               â”ƒ
â”ƒ Successful Downloads â”ƒ 5                          â”ƒ
â”ƒ Failed Documents     â”ƒ 0                          â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
```

## Configuration

Key settings in `config.py`:

- `MAX_CONCURRENCY`: Concurrent requests (default: 4)
- `RATE_LIMIT`: Requests per second (default: 1.0)
- `DATABASE_URL`: PostgreSQL connection string
- `PDF_ROOT`: Export directory for PDF files (optional)

## Database Storage Benefits

âœ… **No File System Dependencies**: All PDFs stored directly in PostgreSQL
âœ… **Simplified Deployment**: Single database contains everything
âœ… **ACID Compliance**: Transactional consistency for PDF downloads
âœ… **Easy Backup/Restore**: Standard PostgreSQL backup tools
âœ… **Scalable**: Handle thousands of small PDFs efficiently
âœ… **Export on Demand**: Extract PDFs to files only when needed

## Technical Details

### Data Extracted

**From Listing Page:**
- Title, Issue Date, FDA Organization
- Topic, Guidance Status, Comment Status
- Direct PDF download URLs

**From Detail Pages:**
- Full document summary
- Docket numbers and URLs
- Federal Register links
- Regulated products and topics

### File Naming

PDFs are stored in database with deterministic filenames for reference:
```
{issue_date}_{title_slug}_{media_id}.pdf
```

Example: `07/31/2025_medical_device_user_fee_small_business_qualificati_176439.pdf`

Use `export-pdfs` command to extract files with these names when needed.

## Development

### Project Structure

```
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ PRD.md              # Product Requirements
â”‚   â””â”€â”€ architecture.md     # Technical Architecture
â”œâ”€â”€ fda_crawler/            # Main package
â”œâ”€â”€ migrate.sql             # Database schema
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ setup.py               # Package setup
```

### Adding Features

The modular design makes it easy to extend:

- Add new parsers in `crawler.py`
- Extend database schema in `models.py`
- Add CLI commands in `cli.py`
- Configure new settings in `config.py`

## Current Limitations

- **JavaScript Content**: Currently uses hardcoded document list since FDA uses dynamic table loading
- **Pagination**: Handles first page only (can be extended)
- **Scale**: Tested with 5 documents (ready for full 2,786+ with proper data source)

## Future Enhancements

- Browser automation for full JavaScript support
- AJAX endpoint discovery for direct API access
- Distributed crawling with Celery
- Full-text PDF extraction and search
- Web dashboard for monitoring

## License

MIT License - see LICENSE file for details.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

**Built with Python 3.8+ â€¢ PostgreSQL â€¢ Modern async/await patterns**
